{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "from trainer.recorder import Recorder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import MAXYEAR, MINYEAR\n",
    "from datetime import datetime, time, timedelta\n",
    "\n",
    "def today():\n",
    "    today = datetime.now().date()\n",
    "    return datetime.combine(today, time.min)\n",
    "def days_ago(days):\n",
    "    day = today()\n",
    "    return day - timedelta(days=days)\n",
    "maxtime = datetime(MAXYEAR, 1, 1)\n",
    "mintime = datetime(MINYEAR, 1, 1)\n",
    "\n",
    "def stdmean(logger: Recorder, *labels, summarize=None):\n",
    "    '''\n",
    "    compute the stdmean of logged data with the given labels across all runs\n",
    "    customized by the `summarize` fn\n",
    "    '''\n",
    "    if summarize is None:\n",
    "        summarize = lambda x: x\n",
    "    series_dict = {}\n",
    "    for run in logger:\n",
    "        run_dict = logger.get_data(run, *labels)\n",
    "        summary_dict = summarize(run_dict)\n",
    "        for new_label in summary_dict:\n",
    "            if new_label not in series_dict:\n",
    "                series_dict[new_label] = []\n",
    "            series_dict[new_label].append(summary_dict[new_label])\n",
    "    stdmean_dict = {}\n",
    "    for label in series_dict:\n",
    "        t = torch.tensor(series_dict[label])\n",
    "        stdmean_dict[label] = [t.mean().item(), t.std().item()]\n",
    "    return stdmean_dict\n",
    "\n",
    "def stdmean_acc(logger: Recorder):\n",
    "    def get_acc(val_test):\n",
    "        val_acc = 100 * torch.tensor(val_test['val/acc'])\n",
    "        valid = val_acc.max().item()\n",
    "        test = 100 * val_test['test/acc'][val_acc.argmax()]\n",
    "        return {'val/acc' : valid, 'test/acc': test}\n",
    "    return stdmean(logger, 'val/acc', 'test/acc', summarize=get_acc)\n",
    "\n",
    "import re\n",
    "\n",
    "def select(loggers, filters: dict, time=None):\n",
    "    def match(to_match: dict, filters: dict):\n",
    "        assert isinstance(filters, dict)\n",
    "        for k in filters:\n",
    "            if k not in to_match:\n",
    "                return False\n",
    "            if isinstance(filters[k], dict):\n",
    "                if not match(to_match[k], filters[k]):\n",
    "                    return False\n",
    "            elif isinstance(filters[k], list) or isinstance(filters[k], tuple):\n",
    "                for i, f in enumerate(filters[k]):\n",
    "                    if not match(to_match[k][i], f):\n",
    "                        return False\n",
    "            elif isinstance(filters[k], str) and isinstance(to_match[k], str):\n",
    "                if re.match(filters[k], to_match[k]) is None:\n",
    "                    return False\n",
    "            else:\n",
    "                if to_match[k] != filters[k]:\n",
    "                    return False\n",
    "        return True\n",
    "    filtered = [\n",
    "        logger for logger in loggers if match(logger.info, filters)\n",
    "    ]\n",
    "    if time is not None:\n",
    "        start, end = time\n",
    "        filtered = [\n",
    "            logger for logger in filtered if logger.time < end and logger.time >= start\n",
    "        ]\n",
    "    return filtered\n",
    "\n",
    "import glob\n",
    "def load_logs(pattern):\n",
    "    def load_pkl(fname):\n",
    "        with open(fname, 'rb') as fp:\n",
    "            try:\n",
    "                logs = pickle.load(fp)\n",
    "                logs.fname = fname.split('/')[-1]\n",
    "                logs.time = datetime.fromtimestamp(os.path.getmtime(fname)).replace(microsecond=0)\n",
    "                if not isinstance(logs.info, dict):\n",
    "                    logs.info = vars(logs.info)\n",
    "                return logs\n",
    "            except:\n",
    "                print(\"ignoring log:\", fname)\n",
    "    files = glob.glob(pattern)\n",
    "    files.sort(key=os.path.getmtime)\n",
    "    return [trace for trace in [load_pkl(f) for f in files] if trace is not None]\n",
    "\n",
    "def extract_acc_curve(log: Recorder):\n",
    "    acc_keys = ('train/acc', 'val/acc', 'test/acc')\n",
    "    proc_data = {\n",
    "        k: [] for k in acc_keys\n",
    "    }\n",
    "    for run in log:\n",
    "        run_acc = log.get_data(run, *acc_keys)\n",
    "        for k in acc_keys:\n",
    "            proc_data[k].append(run_acc[k])\n",
    "    for k in acc_keys:\n",
    "        proc_data[k] = np.array(proc_data[k])\n",
    "    proc_data['runs'] = log.num_runs\n",
    "    return proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import pchip_interpolate as interpolate\n",
    "\n",
    "def plot_on(ax, xss, yss, epochs=None):\n",
    "    def make_conv_figure(ax, xs, acc_curves, **kwargs):\n",
    "        '''\n",
    "        xs: x-axis values, epochs or training time\n",
    "        acc_curves: y-axis values, accuracy (of multiple runs)\n",
    "        '''\n",
    "        # for i, acc_type in enumerate(('train/acc', 'val/acc')):\n",
    "        # titles = ['validation', 'test']\n",
    "        # for i, acc_type in enumerate(('val/acc', 'test/acc')):\n",
    "        # ax.set_title(titles[i])\n",
    "        # ax.margins(x=0)\n",
    "        steps = xs.shape[0]\n",
    "        if epochs is not None:\n",
    "            acc_curves = acc_curves[:, :(epochs+1)]\n",
    "        if xs is None:\n",
    "            xs = np.arange(1, acc_curves.size(1)+1, 1, dtype=int)\n",
    "        mean = acc_curves.mean(axis=0)\n",
    "        std = acc_curves.std(axis=0)\n",
    "        if np.isnan(std).any():\n",
    "            std[:] = 0\n",
    "        # lower, upper = mean-std, mean+std\n",
    "        # interp_xs = xs\n",
    "        interp_xs = np.arange(xs.min(), xs.max(), (xs.max()-xs.min()) / (1000*steps))\n",
    "        l, r = [(2, 0.0)], [(2, 0.0)]\n",
    "        lower = interpolate(xs, mean-std, interp_xs)\n",
    "        upper = interpolate(xs, mean+std, interp_xs)\n",
    "        mean = interpolate(xs, mean, interp_xs)\n",
    "        ax.plot(interp_xs, mean, marker=',', ls='-', lw=2, **kwargs)\n",
    "        kwargs.pop('label', None)\n",
    "        ax.fill_between(interp_xs, lower, upper, alpha=0.1, **kwargs)\n",
    "\n",
    "    # fig, axs = plt.subplots(1, 2, figsize=(10, 5), sharey='row', squeeze=False)\n",
    "    # fig.tight_layout()\n",
    "    # fig.suptitle(f\"Model Convergence Rate in Epochs\", fontsize=16)\n",
    "    # fig.subplots_adjust(top=0.88)\n",
    "    # # for ax, title in zip(axs[0], ('val/acc', 'test/acc')):\n",
    "    # #     ax.set_title(title, fontsize=20)\n",
    "    # for ax in axs[:,0]:\n",
    "    #     ax.set_ylabel('accuracy', fontsize=16)\n",
    "    # for ax in axs[-1]:\n",
    "    #     ax.set_xlabel('epoch', fontsize=16)\n",
    "\n",
    "    for k in yss:\n",
    "        xs = None if xss is None else xss[k]\n",
    "        make_conv_figure(ax, xs=xs, acc_curves=yss[k], label=k)\n",
    "    # plt.show()\n",
    "    # if save_to is not None:\n",
    "    #     fig.savefig(\n",
    "    #         save_to, bbox_inches = \"tight\"\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = load_logs('../logdir/acc/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive the convergence curve (in wallclock time) of training GraphSAGE on ogbn-papers100M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = select(logs,\n",
    "    {'dataset': {'root': '.*/ogbn_papers100M'},\n",
    "     'model': {'arch': '^sage$', 'num_layers': 3, 'epochs': 30},\n",
    "     'sample': {'train': [{'sampler': 'ns', 'batch_size': 1000}]},\n",
    "    },\n",
    "    # time=(days_ago(30), maxtime)\n",
    ")\n",
    "for log in ns:\n",
    "    print(log.md5)\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "ns = ns[-1]\n",
    "\n",
    "rnd = select(logs,\n",
    "    {'dataset': {'root': '.*/ogbn_papers100M'},\n",
    "     'model': {'arch': '^sage$'},\n",
    "     'sample': {'train': [{\n",
    "         'P': 1024, 'batch_size': 128, 'partition': 'rand'\n",
    "        }]},\n",
    "    },\n",
    "    # time=(days_ago(30), maxtime)\n",
    ")\n",
    "for log in rnd:\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "rnd = rnd[-1]\n",
    "\n",
    "hbx1 = select(logs,\n",
    "    {'dataset': {'root': '.*/ogbn_papers100M'},\n",
    "     'model': {'arch': '^sage$'},\n",
    "     'sample': {'train': [{\n",
    "         'P': 1024, 'batch_size': 128, 'partition': 'fennel-wlb',\n",
    "          'num_repeats': 1, 'pivots': True,\n",
    "        }]},\n",
    "    },\n",
    "    # time=(days_ago(30), maxtime)\n",
    ")\n",
    "for log in hbx1:\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "hbx1 = hbx1[-1]\n",
    "\n",
    "hbx2 = select(logs,\n",
    "    {'dataset': {'root': '.*/ogbn_papers100M'},\n",
    "     'model': {'arch': '^sage$'},\n",
    "     'sample': {'train': [{\n",
    "         'P': 1024, 'batch_size': 128, 'partition': 'fennel-wlb',\n",
    "         'num_repeats': 2, 'pivots': True\n",
    "        }]},\n",
    "    },\n",
    "    # time=(days_ago(30), maxtime)\n",
    ")\n",
    "for log in hbx2:\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "hbx2 = hbx2[-1]\n",
    "\n",
    "hbp0 = select(logs,\n",
    "    {'dataset': {'root': '.*/ogbn_papers100M'},\n",
    "     'model': {'arch': '^sage$'},\n",
    "     'sample': {'train': [{\n",
    "         'P': 1024, 'batch_size': 128, 'partition': 'fennel-wlb',\n",
    "         'num_repeats': 2, 'pivots': False\n",
    "        }]},\n",
    "    },\n",
    "    # time=(days_ago(30), maxtime)\n",
    ")\n",
    "for log in hbp0:\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "hbp0 = hbp0[-1]\n",
    "\n",
    "acc_series = {\n",
    "    'NS-Ext': extract_acc_curve(ns),\n",
    "    'HB-rand': extract_acc_curve(rnd),\n",
    "    'HB-ours(r=1)': extract_acc_curve(hbx2),\n",
    "    'HB-ours(r=0)': extract_acc_curve(hbx1),\n",
    "    # 'HB-ours(p=0)': extract_acc_curve(hbp0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google sheets, local machine with 32GB\n",
    "epochs = 30\n",
    "epoch_time = {\n",
    "    'NS-Ext': 181,\n",
    "    # 85 epochs\n",
    "    'HB-rand': 48,\n",
    "    # 50 epochs\n",
    "    'HB-ours(r=1)': 87,\n",
    "    # 75 epochs\n",
    "    'HB-ours(r=0)': 59,\n",
    "    # 75 epochs\n",
    "    # 'HB-ours(p=0)': 55,\n",
    "}\n",
    "# start from zeros\n",
    "train_time = {\n",
    "    k : np.arange(0, epochs+1, step=1) * epoch_time[k] for k in epoch_time\n",
    "}\n",
    "val_acc = {\n",
    "    k : np.concatenate([\n",
    "        np.ones((acc_series[k]['runs'], 1))*.2,\n",
    "        acc_series[k]['val/acc']\n",
    "    ], axis=1) for k in epoch_time\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(6, 5), squeeze=False)\n",
    "fig.tight_layout()\n",
    "ax = axs[0][0]\n",
    "# fig.suptitle(f\"Model Convergence Rate in Epochs\", fontsize=16)\n",
    "# fig.subplots_adjust(top=0.88)\n",
    "# for ax, title in zip(axs[0], ('val/acc', 'test/acc')):\n",
    "#     ax.set_title(title, fontsize=20)\n",
    "ax.set_title('Model Convergence in Wallclock Time', fontsize=16)\n",
    "ax.set_ylabel('Validation Acc', fontsize=16)\n",
    "ax.set_xlabel('Training Time', fontsize=16)\n",
    "ax.set_xlim([-200,4000])\n",
    "ax.set_ylim([0.6,0.7])\n",
    "\n",
    "plot_on(ax, train_time, val_acc, epochs=epochs)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive the convergence curve (in wallclock time) of training GraphSAGE on mag240m-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ns = select(logs,\n",
    "    {'dataset': {'root': '.*/mag240m_c'},\n",
    "     'model': {'arch': '^sage$', 'num_layers': 3, 'epochs': 30},\n",
    "     'sample': {'train': [{'sampler': 'ns', 'batch_size': 1000}]},\n",
    "    },\n",
    "    # time=(days_ago(30), maxtime)\n",
    ")\n",
    "for log in ns:\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "ns = ns[-1]\n",
    "\n",
    "rnd = select(logs,\n",
    "    {'dataset': {'root': '.*/mag240m_c'},\n",
    "     'model': {'arch': '^sage$'},\n",
    "     'sample': {'train': [{\n",
    "         'P': 1024, 'batch_size': 64, 'partition': 'rand'\n",
    "        }]},\n",
    "    },\n",
    "    # time=(days_ago(30), maxtime)\n",
    ")\n",
    "for log in rnd:\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "rnd = rnd[-1]\n",
    "\n",
    "hbx1 = select(logs,\n",
    "    {'dataset': {'root': '.*/mag240m_c'},\n",
    "     'model': {'arch': '^sage$'},\n",
    "     'sample': {'train': [{\n",
    "         'P': 1024, 'batch_size': 64, 'partition': 'fennel-wlb',\n",
    "          'num_repeats': 1, 'pivots': True,\n",
    "        }]},\n",
    "    },\n",
    "    # time=(days_ago(30), maxtime)\n",
    ")\n",
    "for log in hbx1:\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "hbx1 = hbx1[-1]\n",
    "\n",
    "hbx2 = select(logs,\n",
    "    {'dataset': {'root': '.*/mag240m_c'},\n",
    "     'model': {'arch': '^sage$'},\n",
    "     'sample': {'train': [{\n",
    "         'P': 1024, 'batch_size': 64, 'partition': 'fennel-wlb',\n",
    "          'num_repeats': 2, 'pivots': True,\n",
    "        }]},\n",
    "    },\n",
    "    # time=(days_ago(30), maxtime)\n",
    ")\n",
    "for log in hbx2:\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "hbx2 = hbx2[-1]\n",
    "\n",
    "hbp0 = select(logs,\n",
    "    {'dataset': {'root': '.*/mag240m_c'},\n",
    "     'model': {'arch': '^sage$'},\n",
    "     'sample': {'train': [{\n",
    "         'P': 1024, 'batch_size': 64, 'partition': 'fennel-wlb',\n",
    "         'num_repeats': 2, 'pivots': False,\n",
    "        }]},\n",
    "    },\n",
    ")\n",
    "for log in hbp0:\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "hbp0 = hbp0[-1]\n",
    "\n",
    "acc_series = {\n",
    "    'NS-Ext': extract_acc_curve(ns),\n",
    "    'HB-rand': extract_acc_curve(rnd),\n",
    "    'HB-ours(r=1)': extract_acc_curve(hbx2),\n",
    "    'HB-ours(r=0)': extract_acc_curve(hbx1),\n",
    "    'HB-ours(p=0)': extract_acc_curve(hbp0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google sheets, local machine with 32GB\n",
    "epochs = 30\n",
    "epoch_time = {\n",
    "    'NS-Ext': 610,\n",
    "    'HB-rand': 82,\n",
    "    'HB-ours(r=1)': 148,\n",
    "    'HB-ours(r=0)': 100,\n",
    "}\n",
    "# start from zeros\n",
    "train_time = {\n",
    "    k : np.arange(0, epochs+1, step=1) * epoch_time[k] for k in epoch_time\n",
    "}\n",
    "val_acc = {\n",
    "    k : np.concatenate([\n",
    "        np.ones((acc_series[k]['runs'], 1))*.2,\n",
    "        acc_series[k]['val/acc']\n",
    "    ], axis=1) for k in epoch_time\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(6, 5), squeeze=False)\n",
    "fig.tight_layout()\n",
    "ax = axs[0][0]\n",
    "# fig.suptitle(f\"Model Convergence Rate in Epochs\", fontsize=16)\n",
    "# fig.subplots_adjust(top=0.88)\n",
    "# for ax, title in zip(axs[0], ('val/acc', 'test/acc')):\n",
    "#     ax.set_title(title, fontsize=20)\n",
    "ax.set_title('Model Convergence in Wallclock Time', fontsize=16)\n",
    "ax.set_ylabel('Validation Acc', fontsize=16)\n",
    "ax.set_xlabel('Training Time', fontsize=16)\n",
    "ax.set_xlim([-200,8000])\n",
    "ax.set_ylim([0.56,0.66])\n",
    "\n",
    "plot_on(ax, train_time, val_acc, epochs=epochs)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive the convergence curve (in wallclock time) of training GraphSAGE on mag240m (full dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = select(logs,\n",
    "    {'dataset': {'root': '.*/mag240m$'},\n",
    "     'model': {'arch': '^sage$', 'num_layers': 3, 'epochs': 30},\n",
    "     'sample': {'train': [{'sampler': 'ns', 'batch_size': 1000}]},\n",
    "    },\n",
    ")\n",
    "for log in ns:\n",
    "    print(log.info['sample']['train'])\n",
    "    print(log.stdmean(), \"\\n\")\n",
    "ns = ns[-1]\n",
    "print(ns.log[2]['train/loss'])\n",
    "print(ns.log[2]['val/acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: ogbn-arxiv with 20% nodes selected randomly as the training set, the remaining split in half as validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ogbn_arxiv_r with 20% random split for training nodes\n",
    "logs = load_logs('../logdir/acc/*')\n",
    "has_pivots = True\n",
    "repeats = 1\n",
    "\n",
    "ns = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv_r'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'sampler': 'ns'}]},\n",
    "    })\n",
    "for log in ns:\n",
    "    print(log.time,  log.info['sample'], log.stdmean())\n",
    "\n",
    "mts_tb = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv_r'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'metis-tb', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "print()\n",
    "for log in mts_tb:\n",
    "    print(log.time,  log.info['sample'], log.stdmean())\n",
    "\n",
    "mts_wtb = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv_r'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'metis-wtb', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "print()\n",
    "for log in mts_wtb:\n",
    "    print(log.time,  log.info['sample'], log.stdmean())\n",
    "\n",
    "mts_c = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv_r'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'metis', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "print()\n",
    "for log in mts_c:\n",
    "    print(log.time,  log.info['sample'], log.stdmean())\n",
    "\n",
    "print()\n",
    "mts_w = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv_r'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'metis-w', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "for log in mts_w:\n",
    "    print(log.time,  log.info['sample'], log.stdmean())\n",
    "\n",
    "# print()\n",
    "# fnlvnl = select(logs, {\n",
    "#     'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv_r'},\n",
    "#     'model': {'arch': 'sage', 'epochs': 100},\n",
    "#     'sample': {'train': [{'partition': 'fennel-vnl', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "#     })\n",
    "# for log in fnlvnl:\n",
    "#     print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "print()\n",
    "fnl = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv_r'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'fennel', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "for log in fnl:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "fnllb = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv_r'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'fennel-lb', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "print()\n",
    "for log in fnllb:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "fnlwlb = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv_r'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'fennel-wlb', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "print()\n",
    "for log in fnlwlb:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "rnd = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv_r'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'rand', 'num_repeats': 1}]},\n",
    "    })\n",
    "print()\n",
    "for log in rnd:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "def key_fn(info):\n",
    "    cluster_info = info['sample']['train'][0]\n",
    "    return cluster_info['P'], cluster_info['batch_size'], cluster_info['num_repeats']\n",
    "acc_series = {\n",
    "    'NS (in-mem)': extract_acc_curve(ns, lambda x: 'NS'),\n",
    "    # 'HB-min-w': extract_acc_curve(mts_w, key_fn),\n",
    "    'HB-min-c': extract_acc_curve(mts_c, key_fn),\n",
    "    # 'HB-min-tb': extract_acc_curve(mts_tb, key_fn),\n",
    "    # 'HB-nlb': extract_acc_curve(fnl, key_fn),\n",
    "    # 'HB-lb': extract_acc_curve(fnllb, key_fn),\n",
    "    'HB-ours': extract_acc_curve(fnlwlb, key_fn),\n",
    "    'Marius': extract_acc_curve(rnd, key_fn),\n",
    "}\n",
    "plot(acc_series, ylim=[0.51, 0.75])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: ogbn-arxiv with standard splits from OGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = load_logs('../logdir/acc/*')\n",
    "has_pivots = True\n",
    "repeats = 1\n",
    "\n",
    "ns = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'sampler': 'ns'}]},\n",
    "    })\n",
    "for log in ns:\n",
    "    print(log.time,  log.info['sample'], log.stdmean())\n",
    "\n",
    "mts_w = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'metis-w', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "for log in mts_w:\n",
    "    print(log.time,  log.info['sample'], log.stdmean())\n",
    "\n",
    "mts = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'metis', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "for log in mts:\n",
    "    print(log.time,  log.info['sample'], log.stdmean())\n",
    "\n",
    "fnl = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'fennel', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "print()\n",
    "for log in fnl:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "fnllb = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'fennel-lb', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "print()\n",
    "for log in fnllb:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "fnlwlb = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'fennel-wlb', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "print()\n",
    "for log in fnllb:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "rnd = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/ogbn_arxiv'},\n",
    "    'model': {'arch': 'sage', 'epochs': 100},\n",
    "    'sample': {'train': [{'partition': 'rand', 'num_repeats': 1}]},\n",
    "    })\n",
    "print()\n",
    "for log in rnd:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "def key_fn(info):\n",
    "    cluster_info = info['sample']['train'][0]\n",
    "    return cluster_info['P'], cluster_info['batch_size'], cluster_info['num_repeats']\n",
    "acc_series = {\n",
    "    'NS (in-mem)': extract_acc_curve(ns, lambda x: 'NS'),\n",
    "    # 'HB-min-w': extract_acc_curve(mts_w, key_fn),\n",
    "    'HB-min-c': extract_acc_curve(mts, key_fn),\n",
    "    'HB-lb': extract_acc_curve(fnllb, key_fn),\n",
    "    'HB-ours': extract_acc_curve(fnlwlb, key_fn),\n",
    "    'Marius': extract_acc_curve(rnd, key_fn),\n",
    "}\n",
    "\n",
    "plot(acc_series, ylim=[0.4, 0.7], epochs=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: ogbn-papers100M, SAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(acc_series, ylim=None, stderr=True, save_to=None, epochs=None):\n",
    "    def make_conv_figure(axs, shfl_method, ylim=[0.58, 0.74], stderr=True, intervals=1):\n",
    "        plt.ylim(ylim)\n",
    "        # for i, acc_type in enumerate(('train/acc', 'val/acc')):\n",
    "        titles = ['train', 'validation']\n",
    "        for i, acc_type in enumerate(('train/acc', 'val/acc')):\n",
    "            ax = axs[i]\n",
    "            ax.set_title(titles[i])\n",
    "            ax.margins(x=0)\n",
    "            ax.grid()\n",
    "            acc_blocks = acc_series[shfl_method]\n",
    "            for block_info in acc_blocks:\n",
    "                acc_curves = torch.tensor(acc_blocks[block_info][acc_type])\n",
    "                if epochs is not None:\n",
    "                    acc_curves = acc_curves[:, :epochs]\n",
    "                xs = range(0, acc_curves.size(1) * intervals + 1, intervals)\n",
    "                mean = acc_curves.mean(dim=0)\n",
    "                std = acc_curves.std(dim=0)\n",
    "                mean = torch.tensor([0] + mean.tolist())\n",
    "                std = torch.tensor([0] + std.tolist())\n",
    "                if std.isnan().any().item():\n",
    "                    std[:] = 0\n",
    "                label = shfl_method\n",
    "                # interp_xs = torch.arange(intervals, mean.size(0), mean.size(0)/1000)\n",
    "                # lower = make_interp_spline(xs, mean-std)(interp_xs)\n",
    "                # upper = make_interp_spline(xs, mean+std)(interp_xs)\n",
    "                # mean = make_interp_spline(xs, mean)(interp_xs)\n",
    "                # ax.plot(interp_xs, mean, marker=',', label=label)\n",
    "                ax.plot(xs, mean, marker=',', label=label)\n",
    "                ax.fill_between(xs, mean-std, mean+std, alpha=0.1, interpolate=True)\n",
    "\n",
    "        ax.legend(fontsize=12)\n",
    "    # fig, axs = plt.subplots(1, 3, figsize=(18, 18), sharey='row', dpi=200)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5), sharey='row', squeeze=False)\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(f\"Model Convergence Rate in Epochs\", fontsize=16)\n",
    "    fig.subplots_adjust(top=0.88)\n",
    "    # for ax, title in zip(axs[0], ('val/acc', 'test/acc')):\n",
    "    #     ax.set_title(title, fontsize=20)\n",
    "    for ax in axs[:,0]:\n",
    "        ax.set_ylabel('accuracy', fontsize=16)\n",
    "    for ax in axs[-1]:\n",
    "        ax.set_xlabel('epoch', fontsize=16)\n",
    "\n",
    "    for k in acc_series:\n",
    "        make_conv_figure(axs[0], shfl_method=k, stderr=stderr, ylim=ylim)\n",
    "    plt.show()\n",
    "    if save_to is not None:\n",
    "        fig.savefig(\n",
    "            save_to, bbox_inches = \"tight\"\n",
    "        )\n",
    "\n",
    "logs = load_logs('../logdir/acc/*')\n",
    "has_pivots = True\n",
    "repeats = 1\n",
    "\n",
    "ns = select(logs, {\n",
    "    'dataset': {'name': 'ogbn-papers100M'},\n",
    "    'model': {'arch': 'sage'},\n",
    "    # 'sample': {'train': [{'sampler': 'ns'}]},\n",
    "    })\n",
    "for log in ns:\n",
    "    print(log.time,  log.info['sample'], log.stdmean(epochs=30))\n",
    "\n",
    "print(ns[0].get_series(0, 'train/time'))\n",
    "# ours = select(logs, {\n",
    "#     'dataset': {'name': 'ogbn-papers100M'},\n",
    "#     'model': {'arch': 'sage'},\n",
    "#     'sample': {'train': [{'partition': 'fennel-w', 'num_repeats': 1}]}\n",
    "#     })\n",
    "# for log in ours:\n",
    "#     print(log.time,  log.info['sample'], log.stdmean(epochs=30))\n",
    "\n",
    "def key_fn(info):\n",
    "    cluster_info = info['sample']['train'][0]\n",
    "    return cluster_info['P'], cluster_info['batch_size'], cluster_info['num_repeats']\n",
    "acc_series = {\n",
    "    'NS (in-mem)': extract_acc_curve(ns, lambda x: 'NS'),\n",
    "    'HB-ours': extract_acc_curve(ours, key_fn),\n",
    "    # 'Marius': extract_acc_urve(rnd, key_fn),\n",
    "}\n",
    "plot(acc_series, ylim=[0.47, 0.74], epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = load_logs('../logdir/acc/*')\n",
    "has_pivots = True\n",
    "repeats = 1\n",
    "\n",
    "ns = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/mag240m_c'},\n",
    "    'model': {'arch': 'sage'},\n",
    "    'sample': {'train': [{'sampler': 'ns'}]},\n",
    "    })\n",
    "for log in ns:\n",
    "    print(log.time,  log.info['sample'], log.stdmean())\n",
    "\n",
    "fnlwlb = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/mag240m_c'},\n",
    "    'model': {'arch': 'sage', 'epochs': 30},\n",
    "    'sample': {'train': [{'partition': 'fennel-wlb', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "print()\n",
    "for log in fnlwlb:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "fnlw = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/mag240m_c'},\n",
    "    'model': {'arch': 'sage', 'epochs': 30},\n",
    "    'sample': {'train': [{'partition': 'fennel-w', 'pivots': has_pivots, 'num_repeats': repeats}]},\n",
    "    })\n",
    "print()\n",
    "for log in fnlw:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "rnd = select(logs, {\n",
    "    'dataset': {'root': '/mnt/md0/hb_datasets/mag240m_c'},\n",
    "    'model': {'arch': 'sage', 'epochs': 30},\n",
    "    'sample': {'train': [{'partition': 'rand', 'num_repeats': 1}]},\n",
    "    })\n",
    "print()\n",
    "for log in rnd:\n",
    "    print(log.time, log.info['sample'], log.stdmean())\n",
    "\n",
    "def key_fn(info):\n",
    "    cluster_info = info['sample']['train'][0]\n",
    "    return cluster_info['P'], cluster_info['batch_size'], cluster_info['num_repeats']\n",
    "acc_series = {\n",
    "    'NS (in-mem)': extract_acc_curve(ns, lambda x: 'NS'),\n",
    "    'HB-ours': extract_acc_curve(fnlwlb, key_fn),\n",
    "    'HB-ours(w)': extract_acc_curve(fnlw, key_fn),\n",
    "    'Rand+p': extract_acc_curve(rnd, key_fn),\n",
    "}\n",
    "plot(acc_series, ylim=[0.51, 0.75])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = load_logs('../logdir/acc/*.pkl')\n",
    "papers = select(logs, {'dataset': {'root': '/mnt/md0/hb_datasets/mag240m_c'}})\n",
    "for trace in papers:\n",
    "    print(trace.info['sample']['train'], trace.info['model'], trace.stdmean(), trace.md5, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_dict = {\n",
    "    'random hier-batching': {\n",
    "        (64,4): 2.2e-3,\n",
    "        (64,8): 2.2e-3,\n",
    "        (64,16): 2.2e-3,\n",
    "    },\n",
    "    'metis hier-batching': {\n",
    "        (64,4): 9.2e-3,\n",
    "        (64,8): 6.9e-3,\n",
    "        (64,16): 4.4e-3,\n",
    "        # (1024,64): 4.2e-3,\n",
    "        # (1024,128): 3.3e-3,\n",
    "        # (1024,256): 2.8e-3,\n",
    "    },\n",
    "    'fennel-LB hier-batching': {\n",
    "        (64,4): 2.2e-3,\n",
    "        (64,8): 2.1e-3,\n",
    "        (64,16): 2.1e-3,\n",
    "        (1024,16): 2.1e-3,\n",
    "    },\n",
    "    'global shuffling': {\n",
    "        (64,8): 2.1e-3,\n",
    "    },\n",
    "    'shuffling once': {\n",
    "        (64,8): 2.1e-3,\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_acc(logs: list[Recorder]):\n",
    "    acc_dict = {}\n",
    "    for log in logs:\n",
    "        info = log.info\n",
    "        block_info = info.num_blocks, info.num_blocks // info.block_ratio\n",
    "        acc_dict[block_info] = stdmean_acc(log)\n",
    "    return acc_dict\n",
    "\n",
    "acc_method_dict = {\n",
    "    'random hier-batching': extract_acc(rd_logs),\n",
    "    'metis hier-batching': extract_acc(hb_logs),\n",
    "    'fennel-LB hier-batching': extract_acc(fl_logs),\n",
    "    'global shuffling': extract_acc(gs_logs),\n",
    "    # 'shuffling once': extract_acc(ss_logs),\n",
    "}\n",
    "\n",
    "style_dict = {\n",
    "    'random hier-batching': {'marker': 'o', 'color': 'green'},\n",
    "    'metis hier-batching': {'marker': '^', 'color': 'blue'},\n",
    "    'fennel-LB hier-batching': {'marker': 'v', 'color': 'brown'},\n",
    "    'global shuffling': {'marker': 'D', 'color': 'red'},\n",
    "    'shuffling once': {'marker': 'h', 'color': 'orange'},\n",
    "}\n",
    "\n",
    "def make_acc_figure(ax, label, ylim=[68, 74], set_label=False):\n",
    "    plt.ylim(ylim)\n",
    "    for shfl_method in acc_method_dict:\n",
    "        acc_dict = acc_method_dict[shfl_method]\n",
    "        _set_label = set_label\n",
    "        for block_info in acc_dict:\n",
    "            if block_info[0] == block_info[1]:\n",
    "                continue\n",
    "            if block_info not in emd_dict[shfl_method]:\n",
    "                continue\n",
    "            mean, std = acc_dict[block_info][label]\n",
    "            emd = emd_dict[shfl_method][block_info]\n",
    "            if _set_label:\n",
    "                ax.errorbar([emd], [mean], yerr=[std], capsize=3, **style_dict[shfl_method], label=shfl_method)\n",
    "                _set_label = False\n",
    "            else:\n",
    "                ax.errorbar([emd], [mean], yerr=[std], capsize=3, **style_dict[shfl_method])\n",
    "    if set_label:\n",
    "        ax.legend(fontsize=12)\n",
    "    ax.set_title(label, fontsize=16)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4), sharex=True, dpi=160)\n",
    "fig.tight_layout()\n",
    "fig.suptitle(f\"Model Accuracy / Mini-batch Label Discrepancy\", fontsize=16)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "make_acc_figure(axs[0], 'val/acc', ylim=[70, 73], set_label=True)\n",
    "make_acc_figure(axs[1], 'test/acc', ylim=[69, 72])\n",
    "axs[0].set_ylabel('accuracy', fontsize=16)\n",
    "fig.text(0.5, -0.04, 'mean discrepancy of label distribution', ha='center', fontsize=16)\n",
    "fig.savefig(\"label_discrepancy.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
