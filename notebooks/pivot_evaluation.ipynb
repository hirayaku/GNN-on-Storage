{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.0.1+cu117 /home/tianhaoh/.local/lib/python3.9/site-packages/torch\n",
      "PyG: 2.3.1 /home/tianhaoh/.local/lib/python3.9/site-packages/torch_geometric\n",
      "CPU parallelism: 12\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "os.chdir(\"..\")\n",
    "import torch, torch_geometric as pyg\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__, *torch.__path__)\n",
    "print(\"PyG:\", pyg.__version__, *pyg.__path__)\n",
    "print(\"CPU parallelism:\", torch.get_num_threads())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=2315598], NID=[169343])\n",
      "train: 90941, val: 29799, test: 48603\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import index_to_mask, mask_to_index\n",
    "from data.ops import scatter_append\n",
    "from data.partitioner import (\n",
    "    RandomNodePartitioner, MetisWeightedPartitioner,\n",
    "    FennelPartitioner, FennelStrataPartitioner, ReFennelPartitioner,\n",
    ")\n",
    "from graphutils.rw import lazy_rw\n",
    "\n",
    "from pathlib import Path\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "path = Path('/opt/datasets/')\n",
    "dataset = PygNodePropPredDataset(\n",
    "    root=path, name='ogbn-arxiv',\n",
    "    pre_transform=T.ToUndirected(),\n",
    "    transform=T.ToSparseTensor()\n",
    ")\n",
    "data = dataset[0]\n",
    "data.NID = torch.arange(0, data.num_nodes, dtype=torch.int32)\n",
    "num_classes = dataset.num_classes\n",
    "print(data)\n",
    "split = dataset.get_idx_split()\n",
    "train_nid, val_nid, test_nid = split['train'], split['valid'], split['test']\n",
    "print(f\"train: {len(train_nid)}, val: {len(val_nid)}, test: {len(test_nid)}\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class FennelStrataOrderPartitioner(FennelStrataPartitioner):\n",
    "    def __init__(self, g, psize, name='Fennel-strata-deg', **kwargs):\n",
    "        super().__init__(g, psize, name=name, **kwargs)\n",
    "        # overwrite node_order\n",
    "        self.node_order = torch.arange(g.size(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.8885e-05, 4.6398e-06, 5.0517e-06,  ..., 4.5261e-06, 4.2238e-06,\n",
      "        3.0427e-06])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "nids = torch.arange(1)\n",
    "n = data.num_nodes\n",
    "init_score = torch.zeros((data.num_nodes,), device=nids.device)\n",
    "init_score[data.NID] = 1 / data.NID.size(0)\n",
    "final_score = lazy_rw(data.adj_t, init_score, k=1)\n",
    "print(final_score)\n",
    "print(final_score.sum())\n",
    "# print(\"score sum:\", final_score.sum(), \"topk sum:\", topk.values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert a graph into a bidirected graph: 0.067 seconds, peak memory: 11.150 GB\n",
      "Construct multi-constraint weights: 0.006 seconds, peak memory: 11.150 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:36:22] /opt/dgl/src/graph/transform/metis_partition_hetero.cc:78: Partition a graph with 169343 nodes and 2315598 edges into 64 parts and get 729069 edge cuts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metis partitioning: 1.825 seconds, peak memory: 11.288 GB\n",
      "ReFennel Run#0\n",
      "ReFennel Run#1\n",
      "ReFennel Run#2\n",
      "ReFennel Run#3\n",
      "ReFennel Run#4\n",
      "ReFennel Run#0\n",
      "ReFennel Run#1\n",
      "ReFennel Run#2\n",
      "ReFennel Run#3\n",
      "ReFennel Run#4\n",
      "1458138\n",
      "979378\n",
      "1132892\n",
      "2279308\n",
      "tensor(0) tensor(169343) 169343\n"
     ]
    }
   ],
   "source": [
    "P = 64\n",
    "nodes = data.NID\n",
    "labels = data.y.flatten().clone()\n",
    "num_labels = labels.int().max() + 0\n",
    "train_mask = index_to_mask(train_nid, data.size(-1))\n",
    "labels[~train_mask] = num_labels\n",
    "FennelP = ReFennelPartitioner(\n",
    "            data, psize=P, slack=1.5, alpha_ratio=0.1, beta=1, runs=5,\n",
    "            base=FennelPartitioner,\n",
    "        )\n",
    "FennelLB = ReFennelPartitioner(\n",
    "            data, psize=P, slack=1.5, alpha_ratio=0.1, beta=1, runs=5,\n",
    "            base=FennelStrataOrderPartitioner,\n",
    "            labels=labels,\n",
    "        )\n",
    "MetisP = MetisWeightedPartitioner(data, psize=P, node_weights=(labels.float()/4).int())\n",
    "RandP = RandomNodePartitioner(data, psize=P)\n",
    "\n",
    "assigns_mts = MetisP.partition()\n",
    "assigns_fnl = FennelP.partition()\n",
    "assigns_flb = FennelLB.partition()\n",
    "assigns_rnd = RandP.partition()\n",
    "parts_mts, ints_mts, _ = scatter_append(-1, assigns_mts, nodes, P)\n",
    "parts_fnl, ints_fnl, _ = scatter_append(-1, assigns_fnl, nodes, P)\n",
    "parts_flb, ints_flb, _ = scatter_append(-1, assigns_flb, nodes, P)\n",
    "parts_rnb, ints_rnd, _ = scatter_append(-1, assigns_rnd, nodes, P)\n",
    "\n",
    "def edge_cuts(data, assigns):\n",
    "    src, dst, _ = data.adj_t.coo()\n",
    "    return (assigns[src]-assigns[dst] != 0).int().sum().item()\n",
    "print(edge_cuts(data, assigns_mts))\n",
    "print(edge_cuts(data, assigns_fnl))\n",
    "print(edge_cuts(data, assigns_flb))\n",
    "print(edge_cuts(data, assigns_rnd))\n",
    "print(ints_flb[0], ints_flb[-1], data.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cuda()\n",
    "train_cuda = train_nid.cuda()\n",
    "\n",
    "def influential(data, nids, k=3, topk=100):\n",
    "    init_score = torch.zeros((data.num_nodes,), device=nids.device)\n",
    "    init_score[nids] = 1 / nids.size(0)\n",
    "    final_score = lazy_rw(data.adj_t, init_score, k=k)\n",
    "    topk = final_score.cpu().topk(topk)\n",
    "    # print(\"score sum:\", final_score.sum(), \"topk sum:\", topk.values.sum())\n",
    "    return topk.indices, topk.values\n",
    "\n",
    "def importance(data, nids, k=3, topk=100):\n",
    "    init_score = torch.zeros((data.num_nodes,), device=nids.device)\n",
    "    init_score[nids] = 1 / nids.size(0)\n",
    "    final_score = torch.zeros_like(init_score)\n",
    "    score = init_score\n",
    "    for _ in range(k):\n",
    "        score = lazy_rw(data.adj_t, score, k=1)\n",
    "        final_score += score\n",
    "    topk = final_score.cpu().topk(topk)\n",
    "    # print(\"score sum:\", final_score.sum(), \"topk sum:\", topk.values.sum())\n",
    "    return topk.indices, topk.values\n",
    "\n",
    "int_starts, int_ends = ints_flb[:-1], ints_flb[1:]\n",
    "node_parts = [parts_flb[int_starts[i] : int_ends[i]] for i in range(P)]\n",
    "train_parts = [node_parts[i][train_mask[node_parts[i]]] for i in range(P)]\n",
    "\n",
    "fnl_starts, fnl_ends = ints_fnl[:-1], ints_fnl[1:]\n",
    "fnl_parts = [parts_fnl[fnl_starts[i] : fnl_ends[i]] for i in range(P)]\n",
    "train_fnl_parts = [fnl_parts[i][train_mask[fnl_parts[i]]] for i in range(P)]\n",
    "\n",
    "mts_starts, mts_ends = ints_mts[:-1], ints_mts[1:]\n",
    "mts_parts = [parts_mts[mts_starts[i] : mts_ends[i]] for i in range(P)]\n",
    "train_mts_parts = [mts_parts[i][train_mask[mts_parts[i]]] for i in range(P)]\n",
    "\n",
    "# compare with random partitioning\n",
    "rnd_starts, rnd_ends = ints_rnd[:-1], ints_rnd[1:]\n",
    "rnd_parts = [parts_rnb[rnd_starts[i] : rnd_ends[i]] for i in range(P)]\n",
    "train_rnd_parts = [rnd_parts[i][train_mask[rnd_parts[i]]] for i in range(P)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_topk is a list of topk influential nodes for each training node\n",
    "import os\n",
    "if os.path.exists('notebooks/train_topk.pt'):\n",
    "    train_topk, train_topk_scores = torch.load('notebooks/train_topk.pt')\n",
    "else:\n",
    "    train_topk = torch.empty((train_nid.size(0), 100), dtype=torch.long)\n",
    "    train_topk_scores = torch.empty((train_nid.size(0), 100), dtype=torch.float)\n",
    "    from tqdm import tqdm\n",
    "    for i, t in enumerate(tqdm(train_cuda)):\n",
    "        t_topk = influential(data, torch.tensor([t], device='cuda'), topk=100)\n",
    "        train_topk[i][:] = t_topk[0]\n",
    "        train_topk_scores[i][:] = t_topk[1]\n",
    "    torch.save([train_topk, train_topk_scores], \"notebooks/train_topk.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def intersect(s1, s2):\n",
    "    return torch.from_numpy(np.intersect1d(s1.numpy(), s2.numpy()))\n",
    "\n",
    "def get_coverage(train_topk, train_parts, node_parts, pivots):\n",
    "    train_idx = torch.arange(train_nid.size(0))\n",
    "    train_map = torch.zeros(data.size(0), dtype=torch.int64)\n",
    "    train_map[train_nid] = train_idx\n",
    "\n",
    "    best = []\n",
    "    coverage = []\n",
    "    for i in range(len(train_parts)):\n",
    "        trains = train_parts[i]\n",
    "        mask = torch.zeros(data.size(0), dtype=torch.bool)\n",
    "        mask[node_parts[i]] = True\n",
    "        mask[pivots] = True\n",
    "        per_part: torch.Tensor = torch.zeros(data.size(0))\n",
    "        scores = 0\n",
    "        mapped = train_map[trains]\n",
    "        for t in mapped:\n",
    "            current_topk = train_topk[t]\n",
    "            current_scores = train_topk_scores[t]\n",
    "            contains_topk = mask[current_topk]\n",
    "            scores += current_scores[contains_topk].sum()\n",
    "            per_part[current_topk] += current_scores\n",
    "        coverage.append(scores / trains.size(0))\n",
    "        best.append(per_part.topk(node_parts[i].size(0)).values.sum().item() / trains.size(0))\n",
    "\n",
    "    print(\"done\")\n",
    "    return coverage, best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "pivots, _ = importance(data, train_cuda, k=3, topk=int(data.size(0)/32))\n",
    "hubs, _ = influential(data, data.NID, k=1, topk=int(data.size(0)/32))\n",
    "empty = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "# Fennel-LB\n",
    "pivots_cov, pivots_best = get_coverage(train_topk, train_parts, node_parts, pivots)\n",
    "hubs_cov, hubs_best = get_coverage(train_topk, train_parts, node_parts, hubs)\n",
    "nop_cov, nop_best = get_coverage(train_topk, train_parts, node_parts, empty)\n",
    "\n",
    "# pivots_fnl_cov, _ = get_coverage(train_topk, train_fnl_parts, fnl_parts, pivots)\n",
    "# hubs_fnl_cov, _ = get_coverage(train_topk, train_fnl_parts, fnl_parts, hubs)\n",
    "# nop_fnl_cov, _ = get_coverage(train_topk, train_fnl_parts, fnl_parts, empty)\n",
    "\n",
    "pivots_mts_cov, pivots_mts_best = get_coverage(train_topk, train_mts_parts, mts_parts, pivots)\n",
    "mts_cov, mts_best = get_coverage(train_topk, train_mts_parts, mts_parts, empty)\n",
    "pivots_rnd_cov, pivots_rnd_best = get_coverage(train_topk, train_rnd_parts, rnd_parts, pivots)\n",
    "nop_rnd_cov, nop_rnd_best = get_coverage(train_topk, train_rnd_parts, rnd_parts, empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random:\t\t\t(0.19603095948696136, 0.41245684027671814)\n",
      " random+pivot:\t\t(0.37055063247680664, 0.41245684027671814)\n",
      " fennel-LB:\t\t(0.5217406153678894, 0.6659396290779114)\n",
      " fennel-LB+pivot:\t(0.6288290619850159, 0.6659396290779114)\n",
      " fennel-LB+hubs:\t(0.6199474930763245, 0.6659396290779114)\n",
      " metis:\t\t\t(0.5713961720466614, 0.708052396774292)\n",
      " metis+pivots:\t\t(0.6705724000930786, 0.708052396774292)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mean_std(hits):\n",
    "    tensor = torch.tensor(hits)\n",
    "    return torch.mean(tensor).item() # , torch.std(tensor).item()\n",
    "print(\n",
    "    f\"random:\\t\\t\\t{mean_std(nop_rnd_cov), mean_std(nop_rnd_best)}\\n\",\n",
    "    f\"random+pivot:\\t\\t{mean_std(pivots_rnd_cov), mean_std(pivots_rnd_best)}\\n\",\n",
    "    f\"fennel-LB:\\t\\t{mean_std(nop_cov), mean_std(nop_best)}\\n\",\n",
    "    f\"fennel-LB+pivot:\\t{mean_std(pivots_cov), mean_std(pivots_best)}\\n\",\n",
    "    f\"fennel-LB+hubs:\\t{mean_std(hubs_cov), mean_std(pivots_best)}\\n\",\n",
    "    f\"metis:\\t\\t\\t{mean_std(mts_cov), mean_std(mts_best)}\\n\",\n",
    "    f\"metis+pivots:\\t\\t{mean_std(pivots_mts_cov), mean_std(pivots_mts_best)}\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = torch.arange(train_nid.size(0))\n",
    "train_map = torch.zeros(data.size(0), dtype=torch.int64)\n",
    "train_map[train_nid] = train_idx\n",
    "\n",
    "def union(tensor_list, exclude=None):\n",
    "    mask = torch.zeros(data.size(0))\n",
    "    for t in tensor_list:\n",
    "        mask[t] = True\n",
    "    if exclude is not None:\n",
    "        mask[exclude] = False\n",
    "    return mask_to_index(mask)\n",
    "\n",
    "topk = [union(train_topk[train_map[train_parts[i]]]) for i in range(P)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
